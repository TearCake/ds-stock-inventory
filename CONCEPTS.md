# üéì Distributed System Concepts Explained

## Table of Contents
1. [Bully Algorithm (Leader Election)](#bully-algorithm)
2. [Data Replication](#data-replication)
3. [Fault Tolerance](#fault-tolerance)
4. [Consistency Models](#consistency-models)
5. [CAP Theorem](#cap-theorem)
6. [Load Balancing](#load-balancing)

---

## Bully Algorithm

### What is it?
The Bully Algorithm is a **leader election algorithm** used in distributed systems to elect a coordinator (leader) among a group of processes/servers.

### Key Characteristics
- **Assumption**: Each process has a unique numeric ID
- **Rule**: The process with the **highest ID** becomes the leader
- **Triggering**: Election starts when a process detects the leader has failed

### Algorithm Steps

**Initialization:**
1. System starts with all processes active
2. Process with highest ID is automatically the leader

**When Leader Crashes:**

1. **Detection**: A process (P) detects the leader is down

2. **Election Initiation**: P sends ELECTION message to all processes with higher IDs
   ```
   If P = Server 3, send ELECTION to Servers 4, 5, 6
   ```

3. **Response Handling**:
   - **Case A**: No response from higher IDs ‚Üí P declares itself leader
   - **Case B**: Receives response ‚Üí P waits for COORDINATOR message

4. **Higher Process Takes Over**:
   - Higher ID process receives ELECTION
   - Sends OK response to initiator
   - Starts its own election with even higher IDs

5. **New Leader Announcement**:
   - Highest active ID declares itself leader
   - Sends COORDINATOR message to all lower IDs
   - All processes accept the new leader

### Example Scenario

**Setup**: 6 servers (1-6), Server 6 is leader

**Server 6 crashes:**
```
Step 1: Server 5 detects crash, sends ELECTION to Server 6
Step 2: No response from Server 6
Step 3: Server 5 declares itself new leader
Step 4: Server 5 sends COORDINATOR to Servers 1,2,3,4
Step 5: All servers accept Server 5 as new leader
```

**Server 5 crashes while Server 3 is down, then Server 3 recovers:**
```
Step 1: Server 4 detects crash, sends ELECTION to Server 5,6
Step 2: No response
Step 3: Server 4 becomes leader
Step 4: Server 3 restarts, syncs with Server 4
```

### Why "Bully"?
Higher ID processes can "bully" lower ones by forcing them to accept it as leader.

### Advantages
‚úÖ Simple to implement  
‚úÖ Guaranteed to elect highest ID  
‚úÖ Works with any number of failures  

### Disadvantages
‚ùå Many messages in large systems  
‚ùå Higher ID processes have privilege  
‚ùå Multiple simultaneous elections possible  

---

## Data Replication

### What is it?
**Data Replication** is the process of storing copies of data on multiple servers to ensure availability and fault tolerance.

### Types of Replication

**1. Synchronous Replication** (Used in this project)
- Leader waits for acknowledgment from all replicas
- Ensures strong consistency
- Slower but more reliable

**2. Asynchronous Replication**
- Leader doesn't wait for replicas
- Faster but risk of inconsistency
- Eventual consistency

### How It Works in This Project

**Order Placement Example:**
```javascript
1. Client sends order to system
2. Request routed to LEADER server
3. Leader processes order:
   - Creates order object
   - Updates product stock
   - Adds to local orders array
4. Leader replicates to ALL ACTIVE followers:
   - Server 1: ‚úì Replicated
   - Server 2: ‚úì Replicated
   - Server 3: üí• Crashed (skipped)
   - Server 4: ‚úì Replicated
   - Server 5: ‚úì Replicated
5. Leader acknowledges to client
6. All active servers now have the order
```

### Replication Process Flow
```
[Client] ‚Üí [Leader] ‚Üí [Process] ‚Üí [Replicate] ‚Üí [Followers]
                ‚Üì
            [Acknowledge]
                ‚Üì
            [Client]
```

### Benefits
‚úÖ **High Availability**: Data available even if servers fail  
‚úÖ **Fault Tolerance**: No single point of failure  
‚úÖ **Load Distribution**: Read requests can go to any server  
‚úÖ **Data Durability**: Multiple copies prevent data loss  

### Challenges
‚ùå **Network Overhead**: Replication consumes bandwidth  
‚ùå **Consistency**: Keeping all copies synchronized  
‚ùå **Conflict Resolution**: Handling concurrent writes  

---

## Fault Tolerance

### What is it?
**Fault Tolerance** is the ability of a system to continue operating correctly even when some components fail.

### Failure Scenarios

**1. Non-Leader Server Crashes**
```
Before: Servers 1,2,3,4,5,6 (Leader=6)
Crash:  Server 3 fails
After:  System continues normally
        Replication goes to 1,2,4,5,6
        No data loss
```

**2. Leader Server Crashes**
```
Before: Servers 1,2,3,4,5,6 (Leader=6)
Crash:  Server 6 fails
Action: Bully Algorithm triggered
After:  Server 5 becomes new leader
        System continues with new leader
```

**3. Multiple Servers Crash**
```
Before: Servers 1,2,3,4,5,6 (Leader=6)
Crash:  Servers 2,3,6 fail
Action: Server 5 elected leader
After:  System operates with 1,4,5
        Data preserved on remaining servers
```

### Fault Tolerance Mechanisms

**1. Redundancy**
- Multiple server instances
- Data replicated across all
- No single point of failure

**2. Automatic Failover**
- Leader election when leader fails
- Automatic rerouting of requests
- Transparent to clients

**3. Data Synchronization**
- Crashed servers sync on restart
- Leader provides current state
- Ensures consistency

**4. Health Monitoring**
- Continuous server monitoring
- Failure detection
- Automatic recovery initiation

### In This Project
```javascript
// Fault tolerance demonstrated:
1. Crash any server ‚Üí System continues
2. Crash leader ‚Üí New leader elected automatically
3. Place orders ‚Üí Still processed successfully
4. Restart server ‚Üí Syncs and rejoins
```

---

## Consistency Models

### What is Consistency?
**Consistency** ensures all servers see the same data at the same time.

### Strong Consistency (Used in this project)
- **Guarantee**: All reads return the most recent write
- **How**: All writes go through leader, replicated before acknowledgment
- **Trade-off**: Slower writes, but guaranteed correctness

```javascript
Example:
1. Client updates Product Stock: 100 ‚Üí 95
2. Leader updates local: Stock = 95
3. Leader replicates to all active servers
4. All servers acknowledge
5. Leader confirms to client
6. Now ALL servers show Stock = 95
```

### Eventual Consistency
- **Guarantee**: Eventually all servers will have the same data
- **How**: Writes propagate asynchronously
- **Trade-off**: Faster writes, but temporary inconsistencies possible

### Why Strong Consistency Matters for Inventory
```
Scenario: Product has Stock = 1

Without Strong Consistency:
- Client A reads from Server 1: Stock = 1 ‚úì
- Client B reads from Server 2: Stock = 1 ‚úì
- Both place orders simultaneously
- Result: -1 stock (oversold!) ‚ùå

With Strong Consistency:
- Client A order ‚Üí Leader processes ‚Üí Stock = 0
- Replicated to all servers immediately
- Client B reads: Stock = 0
- Order rejected (out of stock)
- Result: Correct! ‚úì
```

---

## CAP Theorem

### What is CAP?
**CAP Theorem** states that a distributed system can only guarantee **2 out of 3** properties:

**C - Consistency**: All nodes see the same data  
**A - Availability**: Every request gets a response  
**P - Partition Tolerance**: System works despite network failures  

### Trade-offs

**CP (Consistency + Partition Tolerance)**
- Sacrifice: Availability
- Example: Banking systems
- This project's approach

**AP (Availability + Partition Tolerance)**
- Sacrifice: Consistency
- Example: Social media feeds
- Focus on uptime

**CA (Consistency + Availability)**
- Sacrifice: Partition Tolerance
- Example: Single-datacenter databases
- Not realistic for distributed systems

### This Project's Choice: CP

```javascript
Why CP (Consistency + Partition Tolerance)?

Consistency Priority:
- Inventory must be accurate
- No overselling products
- Financial transactions require correctness

Partition Tolerance:
- Servers can fail (simulated crashes)
- Network can partition
- System handles it gracefully

Availability Impact:
- If ALL servers crash ‚Üí System unavailable
- But with replicas, highly unlikely
- Acceptable trade-off for correctness
```

### Real-World CAP Examples

**CP Systems**: Banks, Stock Trading, Inventory Management  
**AP Systems**: DNS, Caching systems, Social media  
**CA Systems**: Traditional RDBMS (single instance)  

---

## Summary

### Key Takeaways

1. **Bully Algorithm**: Simple, effective leader election for small-medium systems
2. **Replication**: Essential for availability and fault tolerance
3. **Fault Tolerance**: System survives individual component failures
4. **Strong Consistency**: Critical for financial/inventory applications
5. **CAP Theorem**: Understand trade-offs in distributed system design

### Distributed System Principles

‚úÖ **Redundancy** prevents single points of failure  
‚úÖ **Consensus** algorithms coordinate distributed state  
‚úÖ **Replication** ensures data availability  
‚úÖ **Monitoring** detects and handles failures  
‚úÖ **Recovery** mechanisms restore failed components  

### In Practice

This project demonstrates these concepts through:
- 6 server instances (redundancy)
- Bully Algorithm (consensus)
- Synchronous replication (consistency)
- Crash simulation (fault tolerance)
- Real-time logs (observability)

---

**Further Reading:**
- "Designing Data-Intensive Applications" by Martin Kleppmann
- "Distributed Systems" by Maarten van Steen
- Original CAP Theorem paper by Eric Brewer

**Experiment and Learn!** üéì

---

## Load Balancing

### What is it?
**Load Balancing** is the process of distributing incoming requests across multiple servers to optimize resource utilization, maximize throughput, and minimize response time.

### Why Load Balancing?

**Without Load Balancer:**
```
All requests ‚Üí Server 1 (overloaded) üî•
              Server 2 (idle) üò¥
              Server 3 (idle) üò¥
```

**With Load Balancer:**
```
Requests ‚Üí Load Balancer ‚Üí Server 1 ‚úì
                         ‚Üí Server 2 ‚úì
                         ‚Üí Server 3 ‚úì
(Evenly distributed)
```

### Load Balancing Algorithms

#### 1. **Round Robin**
**How it works**: Distributes requests sequentially in circular order

```javascript
Request 1 ‚Üí Server 1
Request 2 ‚Üí Server 2
Request 3 ‚Üí Server 3
Request 4 ‚Üí Server 1 (cycle repeats)
```

**Pros:** ‚úÖ Simple ‚úÖ Equal distribution  
**Cons:** ‚ùå Ignores server load  
**Best for:** Servers with equal capabilities

#### 2. **Least Connections**
**How it works**: Routes to server with fewest active connections

```javascript
Server 1: 5 connections
Server 2: 3 connections ‚Üê New request
Server 3: 7 connections
```

**Pros:** ‚úÖ Considers load ‚úÖ Prevents overloading  
**Cons:** ‚ùå Requires tracking  
**Best for:** Varying request durations

#### 3. **Weighted Distribution**
**How it works**: Assigns weights based on server capacity

```javascript
Server 1: Weight 3 ‚Üí 30% of requests
Server 2: Weight 2 ‚Üí 20% of requests
Server 3: Weight 3 ‚Üí 30% of requests
```

**Pros:** ‚úÖ Leverages capabilities  
**Cons:** ‚ùå Manual configuration  
**Best for:** Different hardware specs

#### 4. **Least Response Time**
**How it works**: Routes to server with lowest response time

```javascript
Server 1: Avg 120ms
Server 2: Avg 45ms  ‚Üê New request
Server 3: Avg 89ms
```

**Pros:** ‚úÖ Performance-based ‚úÖ Best UX  
**Cons:** ‚ùå Complex tracking  
**Best for:** Performance-critical apps

### Load Balancer Features in This Project

**Performance Metrics:**
- CPU Usage (simulated)
- Memory Usage  
- Response Time
- Active Connections
- Throughput
- Queue Depth

**Demo Load Testing:**
- **Light**: 2 orders/sec √ó 10s = 20 orders
- **Medium**: 5 orders/sec √ó 15s = 75 orders
- **Heavy**: 10 orders/sec √ó 20s = 200 orders

**Health Monitoring:** Every 5 seconds
- Server status check
- Metrics update
- Health score calculation
